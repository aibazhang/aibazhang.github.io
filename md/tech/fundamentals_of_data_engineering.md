# Fundamentals of Data Engineering

## Data Engineerng Described

- Data Engineering Lifecycle
  - Generation -> [Ingestion, Transformation, Serving, Storage] -> [Analytics, ML, Reverse ETL]
  - Undercurrents
    - Security
    - Data management
    - DataOps
    - Data architectur
    - Orchestration
    - Software engineering
- Data Engineering Skill and Activitities
  - DE Focused on
    - balancing the simplest and most cost-effective, best-of-breed, services that deliver value to the business
    - create agile data architectures that evolve as new trends emerge
  - DE dosen't do, but should have a good functioning understanding
    - build ML model
    - create reports or dashboards
    - perform data analysis
    - build KPIs
    - develop software applications
- Data Maturity
  - Starting with data
    - the company may have fuzzy, loosely defined goals or no goals
    - DE is usually generalist
    - jumping to ML without building a solid data fundation always get stuck
  - Scaling with data
    - the challenge is crating scalable data architectures
    - DE move from generalists to specialist
    - scaling is not only cluster nodes, storage, or technology but the data engineering team
  - Leading with data
    - The automated pipelines and systems created by DE allow people within the company to do self-service analystics
    - Creting automation for the seamless intruduction and usage of new data
    - Custom tolls -> competitive advantage
- Skill set
  - Busiiness Responsibilities
    - Know how to communicate with notechnical and technical people
    - Understand how to scope and gather business and product requirements
    - Understand the cultural fundations of Agile, DevOps, and DataOps
    - Control costs
    - Learn continuously
  - Technical Responsibilities
    - SQL, Python, Java/Scala, bash
    - develop proficency in secondary programming languages (Go, JavaScript, Rust)
    - Focus on the fundamentals to understand what'not going to change
    - pay attention to ongoing development to know where the field is going
- Type A & B DE
  - Type A
    - Stands for abstraction
    - DE lifecycle by using entirely off-the-shelf products, managed services and tools
    - across industries and at all levels of data maturity
  - Type B
    - Strands for build
    - stage 2 & 3 in maturity
    - initial data use case is unique and mission-critical that custom data tools are required to get started


## Design Good Data Architecture

- Principle
  - Principle 1: Choose common components wisely
    - Like Making a Toaster
  - Principle 2: Plan for failure
    - availability
    - reliability
    - recovery time objective
      - The maximum acceptable time for a service or system outage
    - recovery point objective
      - The maximum acceptable data loss
  - Principle 3: Architect for scalability
    - scale up
    - scale down
  - Principle 4: Architecture is leadership
    - high technical competence
    - extremely valuable
  - Principle 5: Always be architecting
    - collaborative and agile
  - Principle 6: Build lossely coupled systems
  - Principle 7: Make reversible decisions
  - Principle 8: Prioritize security
  - Principle 9: Enbrace FinOps
    - cost

## Choosing Technologies Across the Data Engineering Lifecycle

- Team size and capabilities
  - For samll teams or teams with weaker technical chops -> use SaaS tools
  - stick with technologies and workflows with which the team is familiar
- Speed to market
  - choose tools that help you move quickly, reliably, safely, and securely
- Interoperability
  - Ensure seleted tech interacts and operates with other technologies
- Cost optimization and business value
  - opex-first approach centered on the cloud and flexible, pay-as-you-go-technologies
  - consider total ooprtunity cost of ownership
- Today versus the future: immutable versus transitory technologies
  - find the immutable technologies along the data engineering
  - build transitory tools around the immutables
  - evaluate tools every 2 years
- Location (cloud, on prem, hybrid cloud, multicloud)
- Build versus buy
- Monolith versus modular
  - Monoliths are attractive but loss of flexbility, ooprtunity cost, and high-friction development cycles.
- Serverless versus servers
- Optimization, performance, and the benchmark wars
- The undercurrents of the engineering lifecycle

## Data Generation in Source System

- Good diplomacy and relationships with the stakeholders of source systems are crucial
- system stakeholder: builds and maintains the source systems
- Data stakeholders: own and control access to the data you want
- **Data contract**

## Storage

- Raw ingredients
  - HDD
  - SSD
  - RAM
  - Networking
  - Serialization
  - Compression
  - CPU
- Storage systems
  - HDFS
  - RDBMS
  - Cache/memory-based storage
    - Redis
  - Object storage
    - key-value store for immutable data objects
    - don't support random writes or append operations
    - work well for a low rate of update operations, where each operation updates a large volume of data
  - Streaming storage
- Storage abstractions
  - Data lake
  - Data lakehouse
    - Key advantage: interoperability
    - various tools can connect to the metadata layer and read data directly from object storage
  - Data platform
  - Cloud data warehouse
- Big idead and Trends in Storage
  - Data Catalog
    - Centralized metadata store for all data across an organization
  - Data Sharing
  - Schema
    - Schema on write
      - enforces data standards, making data easier to consume and utilize in the future
    - Schema on reading
      - emphasizes flexibility, allowing virtually any data to be written
  - Separation of Compute from Storage
  - Data Storage Lifecycle and Data Retention
  - Single-Tenant Versus Multitenant Storage


## Ingestion

- Key engineering considerations for the ingestion phase
  - Bounded Versus Unbounded Data
  - Frequency
    - Batch -> Micro-batch -> Real-time
  - Synchronous Versus Asynchronous Ingestion
  - Serialization and Deserialization
    - serialization: encoding the data from a source and preparing data structures for transmission and intermediate storage stages
  - Throughput and Scalability
  - Reliability and Durability
  - Payload
    - Kind
    - Shape
    - Size
    - Schema and data types
    - Detecting and handling upstream and downstream schema changes
      - even has ingestion tools to automate the detection of schema changes
      - scheme changes still break pipelines downstream
      - communication between those making schema chagnes is important
    - Schema registries
      - track schema versions and history
  - Metadata
  - Push Versus Pull Versus Patterns
- Batch Ingestion Considerations
  - Snapshot or differential extraction
    - Snapshot: extremely common because of their simplicity
    - Differential updates: minizing network traffic and target storage usage
  - File-based export and ingestion
    - Preparation work is done on the source system side
    - because of security reasons
  - ETL versus ELT
  - Inserts, update, and batch size
    - Consider the balance of insert rate and your database
  - Data migration
    - Biggest challenges: movement of data pipeline connections from the old system to the new one
- Message and Stream Ingestion Considerations
  - Schema evolution
  - Late-arriving data
  - ordering and multiple delivery
  - Replay
  - Time to live
  - Message size
  - Error handling and dead-letter queues
  - Consumer PUll and Push
  - Location
- Ways to ingest data
  - Direct Database Connection
    - ODBC/JDBC
    - Source database <-JSBC- Ingestion process -> Object store
  - Change Data Capture
    - Batch-oriented CDC
    - Continuous CDC
    - CDC and database replication
  - APIs
  - Message Queues and event-streaming platforms
  - Managed Data Connectors
  - Moving data with object storage
  - EDI (electornic data interchange)
    - e.g. A cloud-based email server that saves files onto company object storage as soon as they are received
  - Databases and file export
  - Parctical issues with commone file formats
  - Shell
    - AWS CLI
    - gcloud
  - Webhook
  - Web Interface
  - Web Scraping
  - Data Sharing


## Queries, Modeling, and Transformation

- Queries
  - the life of a query
    - SQL query issued
    - parsing and conversion to bytecode
    - query planning and optimization
    - query execution
    - results returned
  - improving query performance
    - optimize your join strategy and schema
    - use the `EXPLAIN` and understand your query's performance
    - avoid full table scans
    - know how your database handles commits
      - PostgreSQL
        - ACID transactions
        - row locking
      - BigQuery
        - allows only one write operation at a time
      - MongoDB
        - ultra-high write performance
        - suitable for applications the can stand to lose some data
        - not a great fir applications that need to capture exact data and statistics
      - vacuum dead records
      - leverage cached query results
  - queries on streaming data
- Data Modeling
  - A data model represents the way data relates to the real world
  - Normalization
    - Denormalized -> 1NF -> 2NF -> 3NF
    - Techniques for modeling batch analytical data
      - Inmon
        - 3NF
        - Source systems -ETL-> DWH -ETL-> Data marts
      - Kimball
        - star schema
      - Data Vault
        - Satellite 1 -> Hub1 -> Link <- Hub 2 <- Statellite 2
- Transformation
  - Why
    - manipulate, enhance, and save data for downstream use
    - increasing its value in a scalable, reliable, and cost-effective manner
    - multiple tables and datasets -> multiple systems
  - Batch Transformations
    - ETL, ELT, Data pipelines
      - ✅ apply the proper technique on a case-by-case basis as they build data pipelines
      - ❎ standarize on ETL or ELT
    - When to avoid SQL for batch transformations in Spark
      - complex process: procedural programming language is a better fit here
      - word-stemming query will be neither readable nor maintainable
      - resuable libraries are easy to create in Spark
      - Recycle SQL
        - reuse the results of a SQL query by commiting to a table or creating a view (Airflow)
        - reuse of SQL statements (dbt)
    - Optimizing Spark and other procesing frameworks
      - ❎ Python UDFs
      - ✅ Spark-native way to accomplish
        - If you must use UDFs, rewriting them in Scala or Java
      - consider intermixing SQL
    - Update pattern
      - CDC
    - Schema updates
      - columns-oriented DB: updating the schema is easier
      - semistructured data
        - frequently accessed data in the JSON field can be added directly into the schema over time.
    - After MapReduce
      - leveraging memory for transformations will continue to yield gains for the foreseeable future
  - Materialized Views, Federation, and Query Virtualization
    - Materialized Views
      - does some or all of the view computation in advance
    - Federated queries
      - allows an OLAP database to select from an external data source
      - some OLAP systems can convert federated queries into materialized views
  - Streaming Transformations and Processing

## Serving Data for Analytics, Machine Learning, and Reverse ETL

- General Considerations for Serving Data
  - A loss of trust is often a silent death kenll for a data project
  - before you get started
    - Who will use the data, and how will they use it?
    - What do stakeholders expect?
    - How can I collaborate with data stakeholders to understand how the data I'm working with will be used?
- Analytics
  - Business Analytics
    - dashboard
    - report
    - ad hoc analysis
      - impactful -> end up in a report or dashboard
  - Operational Analytics
    - real-time updates
    - e.x. monitoring dashboards
  - Enbedded Analytics
    - end-user-facing dashboards
- ML
  - File Exchange
    - Object storage is useful
  - Database
  - Streaming Systems
  - Query Federation
    - must ensure that the federated queries won't consume excessive resources in the source
  - Data Sharing
    - allowing compaines to share data safely nad securely with each other
  - Semantic and Metrics Layers
    - Metrics Layers: a tool for maintaining and computing business logic
      - live BI tool
      - or software that builds transformation queries
      - e.x. Looker and dbt
  - Serving Data in Notebooks
    - DE play a key role in facilitating the move to scalable cloud infrastructure
- Reverse ETL